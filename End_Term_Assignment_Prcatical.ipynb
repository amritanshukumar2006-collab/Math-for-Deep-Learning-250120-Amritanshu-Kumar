{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyY6PBjZyiOCP72ZZGWgxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amritanshukumar2006-collab/Math-for-Deep-Learning-250120-Amritanshu-Kumar/blob/main/End_Term_Assignment_Prcatical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End Term Assignment Prcatical\n",
        "# **Convolutional Neural Network** by using PyTorch\n",
        "\n",
        "\n",
        "Firstly for the bulding of a Nueral Network from Scratch, we are importing various Modules\n",
        "Let's import PyTorch\n",
        "\n"
      ],
      "metadata": {
        "id": "YOnH3ySfFYiU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_odWCP3dFPHp"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "MGJotl6xHAfL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Perceptron\n",
        "Now, we are creating a single neuron. This neurol performs a non - linear transformation using an Activation Function (Sigmoidal Function).\n",
        "\n",
        "Here,\n",
        "\n",
        "1.   w stands for the weights.\n",
        "2.   b stands for biases.\n",
        "We define neural network components by subclassing `nn.Module`.\n"
      ],
      "metadata": {
        "id": "4QKSBGXlHXc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.w = nn.Parameter(torch.randn(input_size) * 0.01) #The weights are allocated for each nueron correction and layer\n",
        "        self.b = nn.Parameter(torch.zeros(1))#These are the biases\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(torch.matmul(x, self.w) + self.b)  # logits"
      ],
      "metadata": {
        "id": "S3OUQkE0HYdP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The DenseLayer\n",
        "***The Dense layer is the decision-making part.***\n",
        "The DenseLayer here will detect all the features togeter and mixes them all. After this, the netural netwrok predcits the most suitable output.\n",
        "Here, we create a layer by stacking multiple `Perceptron` instances. The output is a vector where each element comes from one `perceptron`."
      ],
      "metadata": {
        "id": "gjxQjMSkIlX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList(\n",
        "            [Perceptron(input_size) for _ in range(output_size)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.stack([p(x) for p in self.layer], dim=1)"
      ],
      "metadata": {
        "id": "mhIBa5ZZItS7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Convolution Layer\n",
        "Here, we are defining the layer.\n",
        "We are using learnable kernels/filters as `nn.Parameter`\n",
        "\n",
        "Here, we are defining the Input terms,\n",
        "\n",
        "N is the Batch Size\n",
        "C is the input channel\n",
        "H_in is the height\n",
        "W_in is the weidth"
      ],
      "metadata": {
        "id": "Qb1A5EaALNMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        # These are the learnable filters\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
        "        )\n",
        "        self.bias = nn.Parameter(torch.zeros(out_channels)) #using it as said\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Convolution via im2col (unfold)\n",
        "        \"\"\"\n",
        "        N, C, H_in, W_in = x.shape\n",
        "\n",
        "        # Ensuring that H_in and W_in are pure Python integers\n",
        "        H_in = int(H_in)\n",
        "        W_in = int(W_in)\n",
        "\n",
        "        K = self.kernel_size # Defining Kernel Size\n",
        "\n",
        "        # Extract sliding windows\n",
        "        # The shape is: (N, C*K*K, L)\n",
        "        x_unfold = F.unfold(\n",
        "            x,\n",
        "            kernel_size=K, #Making the size of kernel k x k\n",
        "            stride=self.stride,\n",
        "            padding=self.padding\n",
        "        )\n",
        "\n",
        "        # Reshape weights: (out_channels, C*K*K)\n",
        "        W = self.weight.view(self.weight.size(0), -1) # Each row of W is a flattened convolution filter.\n",
        "\n",
        "        # Doing the Matrix multiplication\n",
        "        # (N, out_channels, L)\n",
        "        out = torch.matmul(W, x_unfold) + self.bias.view(1, -1, 1)\n",
        "\n",
        "        # Compute output spatial size\n",
        "        H_out = (H_in + 2*self.padding - K) // self.stride + 1 #The height of the output is defined\n",
        "        W_out = (W_in + 2*self.padding - K) // self.stride + 1 #The weidth of the output is defined\n",
        "\n",
        "        # Reshape to feature maps\n",
        "        out = out.view(N, -1, H_out, W_out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "K60R5JmZLNrD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The MaxPoolLayer\n",
        "The MaxPoolLayer is an important layer that reduces the size of the maain information, removing all the unnecessary information making the computation easier and faster.\n",
        "\n",
        "\n",
        "---\n",
        "Using the `nn.Module`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# x_unfold = F.unfold(x, kernel_size=K, stride=S)\n",
        "```\n",
        "Here, this `x_unfold` Extracts k × k patches,\n",
        "flattens them and then stores them as columns.\n",
        "\n",
        "\n",
        "After this the shape becomes (N, C, K*K, L)\n",
        "\n",
        "That needs to be reshaped to (N, C, K*K, L)\n"
      ],
      "metadata": {
        "id": "jZJqjlhWN7CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPoolLayer(nn.Module):\n",
        "    def __init__(self, kernel_size, stride=None):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride if stride else kernel_size\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Pooling via unfold\n",
        "        \"\"\"\n",
        "        N, C, H, W = x.shape\n",
        "        K = self.kernel_size\n",
        "        S = self.stride\n",
        "\n",
        "        # Unfold\n",
        "        x_unfold = F.unfold(x, kernel_size=K, stride=S)\n",
        "        # Shape: (N, C*K*K, L)\n",
        "\n",
        "        # Reshape to (N, C, K*K, L)\n",
        "        x_unfold = x_unfold.view(N, C, K*K, -1)\n",
        "\n",
        "        # Max over pooling window\n",
        "        out, _ = torch.max(x_unfold, dim=2)\n",
        "\n",
        "        # Output size\n",
        "        H_out = (H - K) // S + 1 # Here, we are reshaping the image that we get as the output - Height\n",
        "        W_out = (W - K) // S + 1 # Here, we are reshaping the image that we get as the output -\n",
        "\n",
        "        return out.view(N, C, H_out, W_out)"
      ],
      "metadata": {
        "id": "E7ZfhRymRwzf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Convolutional Neural Network**\n",
        "Here we are putting all the layers together making the final Neural Network"
      ],
      "metadata": {
        "id": "s_mP7yWOTifE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = ConvLayer(1, 8, 3, padding=1)\n",
        "        self.pool1 = MaxPoolLayer(2)\n",
        "\n",
        "        self.conv2 = ConvLayer(8, 16, 3, padding=1)\n",
        "        self.pool2 = MaxPoolLayer(2)\n",
        "\n",
        "        self.fc = DenseLayer(16 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x)) # Here, we are using relu function as the activation function\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = torch.relu(self.conv2(x)) # Here, we are using relu function as the activation function\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "3JVSuioJTiNl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are loading our MNIST Dataset into the batch size of 32"
      ],
      "metadata": {
        "id": "JHKm32fsVrnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32)\n"
      ],
      "metadata": {
        "id": "pYaXrCFhVw4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b0b59c-77fb-492a-fd89-c6d46d975211"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 39.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.15MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.91MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.83MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finally TRAINING our MNIST Dataset**\n"
      ],
      "metadata": {
        "id": "XYQMZFHAXzKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CustomCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8j6F4VJXzf2",
        "outputId": "a4e4c9ad-69d6-4031-88ef-2ed9cf59fa46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 1.6168\n",
            "Epoch 2 | Loss: 1.5771\n",
            "Epoch 3 | Loss: 1.5702\n",
            "Epoch 4 | Loss: 1.5668\n",
            "Epoch 5 | Loss: 1.5642\n",
            "Epoch 6 | Loss: 1.5254\n",
            "Epoch 7 | Loss: 1.4844\n",
            "Epoch 8 | Loss: 1.4825\n",
            "Epoch 9 | Loss: 1.4810\n",
            "Epoch 10 | Loss: 1.4800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally Calculating the **Test Accuracy**"
      ],
      "metadata": {
        "id": "Qa8W0gKQpeZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        out = model(x)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LJk5c1npbXo",
        "outputId": "cbd6862e-5361-4e1a-8b6d-ee345ca69d46"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 98.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing the Performance of CNN with DNN**\n",
        "**Convolutional Neural Network**\n",
        "\n",
        "`Convolutional Neural Network` works better in comparision with the `Deep Neural Network` with the case of the Structured Spatial Data, which comprises of Complex Data Patterns such as images, videos and audios.\n",
        "\n",
        "The CNN has this ability because of having the ability to exploit local spatial correlations through `Convolutional filters`and `Weight sharing`, this reduces the number trainable parameters. This makes the Computation inexpensive and more efiicient.\n",
        "\n",
        "CNN enforces inductive bias,\n",
        "\n",
        "Since, *f(T(x))* = *T(f(x))*\n",
        "\n",
        "**Deep Neural Network**\n",
        "\n",
        "DNNs treat all input features as independent and it lacks of inherent spatial inductive bias (*Unlike the CNN*). The `Deep Neural Network` requires more number of trainable parameters to learn complex Data Patterns, making the computation more expensive and less efficient than CNN."
      ],
      "metadata": {
        "id": "7W8X9WjNbVdc"
      }
    }
  ]
}